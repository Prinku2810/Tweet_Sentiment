# -*- coding: utf-8 -*-
"""Tweets_sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1809j_FnrsdxFvXEc86YGuhBgCqf8fTYO

Install Required Packages
"""

!pip install nltk

!pip install googletrans==3.1.0a0

!pip install gensim==4.2.0

"""Import Libraries"""

import pandas as pd
import json
from googletrans import Translator
import re
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
import os
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import multiprocessing
import gensim
from gensim.models import Word2Vec
from collections import defaultdict
from gensim.models import KeyedVectors
from gensim.test.utils import get_tmpfile
from gensim.models.phrases import Phrases, Phraser
from sklearn.cluster import KMeans

"""Load Dataset"""

pd.options.display.max_colwidth=600

with open('/content/tweets.json') as jfile:
  d = json.load(jfile)

data=pd.DataFrame.from_dict(d,orient = 'index')
print(data.head(2))

"""Translate dataset in to english language"""

translator = Translator()
data["tweet_text"] = data["tweet_text"].apply(lambda x: translator.translate(x, dest="en").text)
data["tweet_author"] = data["tweet_author"].apply(lambda x: translator.translate(x, dest="en").text)
print(data.tail(50))

data.to_json('trans_tweets.json')

with open('/content/trans_tweets.json') as file:
  s = json.load(file)

data=pd.DataFrame.from_dict(s)

"""Remove NUll and Duplicate Values"""

data.drop_duplicates(inplace=True)

data.duplicated().sum()

data.isnull().sum()

data.dropna(axis=0,inplace=True)

data.isnull().sum()

"""Reset index"""

data.index=range(0,data.shape[0])

"""Data Cleaning"""

def remove_hashtags(text):
   pattern = re.compile(r'#[\w]*')
   text = re.sub(pattern, ' ', text)
   return text

def remove_urls(text):
  pattern = re.compile(r'https|RT|@[\w]*')
  text = re.sub(pattern, ' ', text)
  remove_https = re.sub(r'(http\S+:\/\/)?([\da-z\.-]+)\.([a-z\.]{2,6})([\/\w \.-]*)', '', text)
  return remove_https

def remove_punc(text):
  text = re.sub(r"[^a-zA-Z]+", ' ', text) 
  return text

def lower_case(text):
  text=text.lower()
  return text

data['tweet_author']=data['tweet_author'].apply(lambda z: remove_hashtags(z))
data['tweet_text']=data['tweet_text'].apply(lambda z: remove_hashtags(z))

data['tweet_author']=data['tweet_author'].apply(lambda z: remove_urls(z))
data['tweet_text']=data['tweet_text'].apply(lambda z: remove_urls(z))

data['tweet_author']=data['tweet_author'].apply(lambda z: remove_punc(z))
data['tweet_text']=data['tweet_text'].apply(lambda z: remove_punc(z))

data['tweet_author']=data['tweet_author'].apply(lambda z: lower_case(z))
data['tweet_text']=data['tweet_text'].apply(lambda z: lower_case(z))

data['tweet_author']=data['tweet_author'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))
data['tweet_text']=data['tweet_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))

os.listdir('/root/nltk_data/corpora/stopwords/')
stop_words = set(stopwords.words("english"))

def split_t(text):
  text=text.split()
  return text

def remove_stopwords(text):
  words = nltk.word_tokenize(text)
  without_stop_words = " ".join([word for word in words if not word in stop_words])
  return without_stop_words

data['tweet_text'] = data['tweet_text'].apply(lambda x: remove_stopwords(x))

data['tweet_text']=data['tweet_text'].apply(lambda x: split_t(x))

"""Count the Number of most frequent entities"""

count_vectorizer = CountVectorizer(binary = False)
bag_of_words = count_vectorizer.fit_transform(data['tweet_text'])
feature_names = count_vectorizer.get_feature_names()
tokens = pd.DataFrame(bag_of_words.toarray(),columns=feature_names)

freq_entities=tokens.sum(axis = 0, skipna = True)

Objective_1=pd.DataFrame(freq_entities.sort_values(ascending=False))

"""Sentiment analysis using word2vec and kmeans clustering"""

def bigram(sent):
  phrases = Phrases(sent, min_count=1, progress_per=50000)
  bigram = (Phraser(phrases))
  sentences = bigram[sent]
  return sentences

data['Cleantweet'] = data['tweet_text'].apply(lambda x: bigram(x))

sent = [row for row in data.tweet_text]
phrases = Phrases(sent, min_count=1, progress_per=50000)
bigram = (Phraser(phrases))
sentences = bigram[sent]

print(sentences[1])

w2v_model = Word2Vec(min_count=3,
                     window=4,
                     vector_size=300,
                     sample=1e-5, 
                     alpha=0.03, 
                     min_alpha=0.0007, 
                     negative=20,
                     workers=multiprocessing.cpu_count()-1)
w2v_model.build_vocab(data['Cleantweet'], progress_per=50000)

w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)
w2v_model.init_sims(replace=True)

w2v_model.save("word2vec.model")

word_vectors = Word2Vec.load("word2vec.model").wv

words = pd.DataFrame(word_vectors.index_to_key)
words.columns = ['words']
words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])
words['cluster'] =clusters

print(words[words['cluster']==2].tail(50))

print(words[words['cluster']==1].tail(10))
print(words[words['cluster']==0].head(5))

words['cluster'].replace({1:'Positive',0:'Negative',2:'Neutral'},inplace=True)