{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Packages"
      ],
      "metadata": {
        "id": "lOfGzP4NWfA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "052FxkuqWedc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "kGF7KgqwWqA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "XKwuzdSAWRD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNX1arH_VUJT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from googletrans import Translator\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as s\n",
        "from wordcloud import WordCloud\n",
        "from nltk import FreqDist\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "tw_wLH8jXe37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_colwidth=600"
      ],
      "metadata": {
        "id": "TkKMc3u1XhVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/tweets.json') as jfile:\n",
        "  d = json.load(jfile)"
      ],
      "metadata": {
        "id": "MxJPJlH6XlUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.DataFrame.from_dict(d,orient = 'index')\n",
        "print(data.head(2))"
      ],
      "metadata": {
        "id": "4peopO5AXpf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate dataset in to english language"
      ],
      "metadata": {
        "id": "lJ7boxXvYjc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()\n",
        "data[\"tweet_text\"] = data[\"tweet_text\"].apply(lambda x: translator.translate(x, dest=\"en\").text)\n",
        "data[\"tweet_author\"] = data[\"tweet_author\"].apply(lambda x: translator.translate(x, dest=\"en\").text)\n",
        "print(data.tail(50))"
      ],
      "metadata": {
        "id": "boPfoFrlYmSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_json('trans_tweets.json')"
      ],
      "metadata": {
        "id": "N2fN0Gbo18AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/trans_tweets.json') as file:\n",
        "  s = json.load(file)"
      ],
      "metadata": {
        "id": "_DhgphUKzYRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.DataFrame.from_dict(s)\n"
      ],
      "metadata": {
        "id": "UwMt55_CM6Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove NUll and Duplicate Values"
      ],
      "metadata": {
        "id": "KCWHToqeY21-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "AdYSkgBJY8Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "pFuTJ0nWZADX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "iqyhv5LOZG6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "BZvPR4g4ZI_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "rgo-6kKoZJby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reset index"
      ],
      "metadata": {
        "id": "qHFAJu_wZMDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.index=range(0,data.shape[0])"
      ],
      "metadata": {
        "id": "Lhh6mUj4ZONK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "rqbH2PTQZcYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hashtags(text):\n",
        "   pattern = re.compile(r'#[\\w]*')\n",
        "   text = re.sub(pattern, ' ', text)\n",
        "   return text"
      ],
      "metadata": {
        "id": "9jSSxYGzZeBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "  pattern = re.compile(r'https|RT|@[\\w]*')\n",
        "  text = re.sub(pattern, ' ', text)\n",
        "  remove_https = re.sub(r'(http\\S+:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', text)\n",
        "  return remove_https"
      ],
      "metadata": {
        "id": "9Xvwq2T_c-Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(text):\n",
        "  text = re.sub(r\"[^a-zA-Z]+\", ' ', text) \n",
        "  return text"
      ],
      "metadata": {
        "id": "nksEP5_CdQAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_case(text):\n",
        "  text=text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "L4fRyTY9djVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet_author']=data['tweet_author'].apply(lambda z: remove_hashtags(z))\n",
        "data['tweet_text']=data['tweet_text'].apply(lambda z: remove_hashtags(z))\n"
      ],
      "metadata": {
        "id": "bA00Jtxcd8xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet_author']=data['tweet_author'].apply(lambda z: remove_urls(z))\n",
        "data['tweet_text']=data['tweet_text'].apply(lambda z: remove_urls(z))"
      ],
      "metadata": {
        "id": "z_J8-S8AzvcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet_author']=data['tweet_author'].apply(lambda z: remove_punc(z))\n",
        "data['tweet_text']=data['tweet_text'].apply(lambda z: remove_punc(z))"
      ],
      "metadata": {
        "id": "Ul4BIIcbz_cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet_author']=data['tweet_author'].apply(lambda z: lower_case(z))\n",
        "data['tweet_text']=data['tweet_text'].apply(lambda z: lower_case(z))"
      ],
      "metadata": {
        "id": "7JeQZVSM0_q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet_author']=data['tweet_author'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "data['tweet_text']=data['tweet_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))"
      ],
      "metadata": {
        "id": "ox0zMNXX1zJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "os.listdir('/root/nltk_data/corpora/stopwords/')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "ZX6SONlGJrZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_t(text):\n",
        "  text=text.split()\n",
        "  return text"
      ],
      "metadata": {
        "id": "M7HNjwx2jQ11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  words = nltk.word_tokenize(text)\n",
        "  without_stop_words = \" \".join([word for word in words if not word in stop_words])\n",
        "  return without_stop_words"
      ],
      "metadata": {
        "id": "NnUCZHGSJ-mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet_text'] = data['tweet_text'].apply(lambda x: remove_stopwords(x))"
      ],
      "metadata": {
        "id": "7YaJ697qKAX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet_text']=data['tweet_text'].apply(lambda x: split_t(x))"
      ],
      "metadata": {
        "id": "6P81oqmDjZ-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the Number of most frequent entities"
      ],
      "metadata": {
        "id": "djvnY_hKuj_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer # give you the bag of words model\n",
        "import pandas as pd\n",
        "count_vectorizer = CountVectorizer(binary = False)\n",
        "bag_of_words = count_vectorizer.fit_transform(data['tweet_text'])\n",
        "feature_names = count_vectorizer.get_feature_names()\n",
        "tokens = pd.DataFrame(bag_of_words.toarray(),columns=feature_names)"
      ],
      "metadata": {
        "id": "p27b3pdvZuA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_entities=tokens.sum(axis = 0, skipna = True)"
      ],
      "metadata": {
        "id": "D4csIMzYZ9ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Objective_1=pd.DataFrame(freq_entities.sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "vm_wPkZlZ_WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis using word2vec and kmeans clustering"
      ],
      "metadata": {
        "id": "ZRamvxlt2Si8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.2.0"
      ],
      "metadata": {
        "id": "LWEUCVqSKXkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from collections import defaultdict\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models.phrases import Phrases, Phraser"
      ],
      "metadata": {
        "id": "3EJclA_n2ka1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram(sent):\n",
        "  phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
        "  bigram = (Phraser(phrases))\n",
        "  sentences = bigram[sent]\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "JEivg6G-SHfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Cleantweet'] = data['tweet_text'].apply(lambda x: bigram(x))"
      ],
      "metadata": {
        "id": "cnlct7w2SVt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [row for row in data.tweet_text]\n",
        "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
        "bigram = (Phraser(phrases))\n",
        "sentences = bigram[sent]"
      ],
      "metadata": {
        "id": "xxm10zBm2lKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIBc8nKINroH",
        "outputId": "9ec296ff-6133-4ffa-96f7-f885c948d883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['phase', 'acalabrutinib', 'venetoclax', 'trial', 'still', 'recruitment', 'phase_study', 'well', 'venetoclax', 'acalabrutinib', 'works', 'mcl', 'patients', 'either', 'relapsed', 'non', 'respondent', 'initial_therapy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=3,\n",
        "                     window=4,\n",
        "                     vector_size=300,\n",
        "                     sample=1e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=multiprocessing.cpu_count()-1)\n",
        "w2v_model.build_vocab(data['Cleantweet'], progress_per=50000)\n"
      ],
      "metadata": {
        "id": "R1Kkqmm220lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "w2v_model.init_sims(replace=True)\n"
      ],
      "metadata": {
        "id": "4M8Nmr1T2399"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "8EXA2Rlo3Il2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "a6K36o163cMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = Word2Vec.load(\"word2vec.model\").wv"
      ],
      "metadata": {
        "id": "Wmq5a1Zj34Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = pd.DataFrame(word_vectors.index_to_key)\n",
        "words.columns = ['words']\n",
        "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
        "words['cluster'] =clusters\n",
        "\n",
        "print(words[words['cluster']==2].tail(50))"
      ],
      "metadata": {
        "id": "5j0ITrB9kyQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words[words['cluster']==1].tail(10))\n",
        "print(words[words['cluster']==0].head(5))\n"
      ],
      "metadata": {
        "id": "DdTpDLPjk1HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words['cluster'].replace({1:'Positive',0:'Negative',2:'Neutral'},inplace=True)"
      ],
      "metadata": {
        "id": "s4v1-Je7vNX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
